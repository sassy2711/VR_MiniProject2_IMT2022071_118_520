{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T16:28:25.065111Z",
     "iopub.status.busy": "2025-05-13T16:28:25.064617Z",
     "iopub.status.idle": "2025-05-13T17:02:18.386169Z",
     "shell.execute_reply": "2025-05-13T17:02:18.385390Z",
     "shell.execute_reply.started": "2025-05-13T16:28:25.065086Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Without KV Cache: 100%|██████████| 10000/10000 [17:13<00:00,  9.68it/s]\n",
      "With KV Cache: 100%|██████████| 10000/10000 [16:37<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Average time WITHOUT KV Cache: 0.1033 seconds/sample\n",
      "Average time WITH KV Cache  : 0.0997 seconds/sample\n",
      "Answer consistency          : 10000/10000 identical\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "# Fine-tuned model path\n",
    "MODEL_PATH = \"/kaggle/input/fine-tuned-blip-vqa-lora/fine_tuned_blip_vqa_lora\"\n",
    "\n",
    "# Data paths \n",
    "VQA_CSV_PATH = \"/kaggle/input/vqadataset/VQADataset.csv\"\n",
    "ABO_META_PATH = \"/kaggle/input/abo-small/metadata/images.csv\"\n",
    "ABO_IMAGE_PATH = \"/kaggle/input/abo-small/small\"\n",
    "\n",
    "# Number of samples to test\n",
    "NUM_SAMPLES = 10000\n",
    "\n",
    "\n",
    "# LOAD MODEL & PROCESSOR\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# LOAD DATA\n",
    "\n",
    "vqa_df = pd.read_csv(VQA_CSV_PATH)\n",
    "abo_meta = pd.read_csv(ABO_META_PATH)\n",
    "vqa_df = pd.merge(vqa_df, abo_meta[['image_id', 'path']], on='image_id', how='left')\n",
    "vqa_df = vqa_df.dropna(subset=[\"path\"]).reset_index(drop=True)\n",
    "vqa_df = vqa_df.head(NUM_SAMPLES)\n",
    "\n",
    "\n",
    "# INFERENCE FUNCTIONS\n",
    "\n",
    "def run_inference(use_cache: bool):\n",
    "    answers = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _, row in tqdm(vqa_df.iterrows(), total=len(vqa_df), desc=f\"{'With' if use_cache else 'Without'} KV Cache\"):\n",
    "        image_path = os.path.join(ABO_IMAGE_PATH, row['path'])\n",
    "        question = row['question']\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                pixel_values=inputs[\"pixel_values\"],  \n",
    "                use_cache=use_cache,\n",
    "                max_length=32,\n",
    "                num_beams=1,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        answer = processor.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        answers.append(answer)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_time = total_time / len(vqa_df)\n",
    "    return answers, avg_time\n",
    "\n",
    "\n",
    "\n",
    "# RUN BENCHMARK\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "answers_no_cache, time_no_cache = run_inference(use_cache=False)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "answers_with_cache, time_with_cache = run_inference(use_cache=True)\n",
    "\n",
    "# RESULTS\n",
    "\n",
    "# Compare answers \n",
    "num_same = sum([a1 == a2 for a1, a2 in zip(answers_no_cache, answers_with_cache)])\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"Average time WITHOUT KV Cache: {time_no_cache:.4f} seconds/sample\")\n",
    "print(f\"Average time WITH KV Cache  : {time_with_cache:.4f} seconds/sample\")\n",
    "print(f\"Answer consistency          : {num_same}/{NUM_SAMPLES} identical\")\n",
    "print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T06:22:47.987674Z",
     "iopub.status.busy": "2025-05-15T06:22:47.986703Z",
     "iopub.status.idle": "2025-05-15T07:27:26.977000Z",
     "shell.execute_reply": "2025-05-15T07:27:26.975936Z",
     "shell.execute_reply.started": "2025-05-15T06:22:47.987644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference (Run 1): 100%|██████████| 5000/5000 [32:22<00:00,  2.57it/s]\n",
      "Inference (Run 2): 100%|██████████| 5000/5000 [32:13<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Average time (Run 1): 0.3885 seconds/sample\n",
      "Average time (Run 2): 0.3868 seconds/sample\n",
      "Answer consistency  : 5000/5000 identical\n",
      "Inference results saved to /kaggle/working/inference_results.csv\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging for errors\n",
    "logging.basicConfig(filename='/kaggle/working/inference_errors.log', level=logging.ERROR, format='%(asctime)s - %(message)s')\n",
    "\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "# Fine-tuned model path \n",
    "MODEL_PATH = \"/kaggle/input/fine-tuned-vilt-vqa-lora/fine_tuned_vilt_vqa_lora/fine_tuned_vilt_vqa_lora\"\n",
    "\n",
    "# Data paths\n",
    "VQA_CSV_PATH = \"/kaggle/input/vqadataset/VQADataset.csv\"\n",
    "ABO_META_PATH = \"/kaggle/input/abo-small/metadata/images.csv\"\n",
    "ABO_IMAGE_PATH = \"/kaggle/input/abo-small/small\"\n",
    "\n",
    "# Number of samples to test\n",
    "NUM_SAMPLES = 5000\n",
    "\n",
    "# LOAD MODEL & PROCESSOR\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "try:\n",
    "    model = ViltForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load model from {MODEL_PATH}: {e}\")\n",
    "    raise\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get answer vocabulary for mapping logits to answers\n",
    "answer_vocab = model.config.id2label  # Maps index (0-3128) to answer string\n",
    "\n",
    "\n",
    "# LOAD DATA\n",
    "\n",
    "try:\n",
    "    vqa_df = pd.read_csv(VQA_CSV_PATH)\n",
    "    abo_meta = pd.read_csv(ABO_META_PATH)\n",
    "    vqa_df = pd.merge(vqa_df, abo_meta[['image_id', 'path']], on='image_id', how='left')\n",
    "    vqa_df = vqa_df.dropna(subset=[\"path\"]).reset_index(drop=True)\n",
    "    vqa_df = vqa_df.head(NUM_SAMPLES)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load or process dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# INFERENCE FUNCTIONS\n",
    "\n",
    "def run_inference(run_label: str = \"Standard\"):\n",
    "    answers = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _, row in tqdm(vqa_df.iterrows(), total=len(vqa_df), desc=f\"Inference ({run_label})\"):\n",
    "        image_path = os.path.join(ABO_IMAGE_PATH, row['path'])\n",
    "        question = row['question']\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = image.resize((384, 384), Image.Resampling.LANCZOS)  # ViLT expects 384x384\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {image_path}: {e}\")\n",
    "            image = Image.new(\"RGB\", (384, 384), (0, 0, 0))\n",
    "\n",
    "        inputs = processor(\n",
    "            images=image,\n",
    "            text=question,\n",
    "            padding=\"max_length\",\n",
    "            max_length=40,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                pixel_values=inputs[\"pixel_values\"]\n",
    "            )\n",
    "\n",
    "        # Get the predicted answer from logits\n",
    "        logits = outputs.logits  # Shape: [1, 3129]\n",
    "        pred_idx = torch.argmax(logits, dim=-1).item()  # Get index of highest logit\n",
    "        answer = answer_vocab[pred_idx]  # Map index to answer string\n",
    "        answers.append(answer)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_time = total_time / len(vqa_df)\n",
    "    return answers, avg_time\n",
    "\n",
    "\n",
    "# RUN BENCHMARK\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "answers_run1, time_run1 = run_inference(run_label=\"Run 1\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "answers_run2, time_run2 = run_inference(run_label=\"Run 2\")\n",
    "\n",
    "\n",
    "# RESULTS\n",
    "\n",
    "# Compare answers\n",
    "num_same = sum([a1 == a2 for a1, a2 in zip(answers_run1, answers_run2)])\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'question': vqa_df['question'],\n",
    "    'answer_run1': answers_run1,\n",
    "    'answer_run2': answers_run2\n",
    "})\n",
    "results_df.to_csv('/kaggle/working/inference_results.csv', index=False)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"Average time (Run 1): {time_run1:.4f} seconds/sample\")\n",
    "print(f\"Average time (Run 2): {time_run2:.4f} seconds/sample\")\n",
    "print(f\"Answer consistency  : {num_same}/{NUM_SAMPLES} identical\")\n",
    "print(f\"Inference results saved to /kaggle/working/inference_results.csv\")\n",
    "print(\"==============================\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7189537,
     "sourceId": 11472097,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7304749,
     "sourceId": 11641302,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7373154,
     "sourceId": 11745255,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7405817,
     "sourceId": 11793980,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7420679,
     "sourceId": 11814565,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
