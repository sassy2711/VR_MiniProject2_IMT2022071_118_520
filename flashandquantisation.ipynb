{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11472097,"sourceType":"datasetVersion","datasetId":7189537},{"sourceId":11641302,"sourceType":"datasetVersion","datasetId":7304749},{"sourceId":11745255,"sourceType":"datasetVersion","datasetId":7373154},{"sourceId":11796050,"sourceType":"datasetVersion","datasetId":7407365}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install flash-attn --no-build-isolation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:00:07.526123Z","iopub.execute_input":"2025-05-13T18:00:07.526471Z","iopub.status.idle":"2025-05-13T18:00:10.993850Z","shell.execute_reply.started":"2025-05-13T18:00:07.526446Z","shell.execute_reply":"2025-05-13T18:00:10.992977Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: flash-attn in /usr/local/lib/python3.11/dist-packages (2.7.4.post1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom torch import nn\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:22:23.207199Z","iopub.execute_input":"2025-05-13T18:22:23.207865Z","iopub.status.idle":"2025-05-13T18:22:23.212344Z","shell.execute_reply.started":"2025-05-13T18:22:23.207841Z","shell.execute_reply":"2025-05-13T18:22:23.211633Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ===============================\n# CONFIGURATION\n# ===============================\n\nMODEL_PATH = \"/kaggle/input/finetuning/fine_tuned_blip_vqa_lora\"\nVQA_CSV_PATH = \"/kaggle/input/vqadataset/VQADataset.csv\"\nABO_META_PATH = \"/kaggle/input/abo-small/metadata/images.csv\"\nABO_IMAGE_PATH = \"/kaggle/input/abo-small/small\"\nNUM_SAMPLES = 10000  # Reduced for faster testing; adjust as needed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:22:24.948229Z","iopub.execute_input":"2025-05-13T18:22:24.948874Z","iopub.status.idle":"2025-05-13T18:22:24.953051Z","shell.execute_reply.started":"2025-05-13T18:22:24.948850Z","shell.execute_reply":"2025-05-13T18:22:24.952138Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# ===============================\n# LOAD MODEL & PROCESSOR\n# ===============================\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(MODEL_PATH)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:22:26.543345Z","iopub.execute_input":"2025-05-13T18:22:26.544000Z","iopub.status.idle":"2025-05-13T18:22:29.087966Z","shell.execute_reply.started":"2025-05-13T18:22:26.543979Z","shell.execute_reply":"2025-05-13T18:22:29.087000Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"BlipForQuestionAnswering(\n  (vision_model): BlipVisionModel(\n    (embeddings): BlipVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (encoder): BlipEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x BlipEncoderLayer(\n          (self_attn): BlipAttention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (projection): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): BlipMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (text_encoder): BlipTextModel(\n    (embeddings): BlipTextEmbeddings(\n      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): BlipTextEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BlipTextLayer(\n          (attention): BlipTextAttention(\n            (self): BlipTextSelfAttention(\n              (query): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): BlipTextSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (crossattention): BlipTextAttention(\n            (self): BlipTextSelfAttention(\n              (query): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): lora.Linear(\n                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=768, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): BlipTextSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): BlipTextIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BlipTextOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (text_decoder): BlipTextLMHeadModel(\n    (bert): BlipTextModel(\n      (embeddings): BlipTextEmbeddings(\n        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): BlipTextEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BlipTextLayer(\n            (attention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (crossattention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): BlipTextIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BlipTextOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (cls): BlipTextOnlyMLMHead(\n      (predictions): BlipTextLMPredictionHead(\n        (transform): BlipTextPredictionHeadTransform(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (transform_act_fn): GELUActivation()\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Load CSVs\nvqa_df = pd.read_csv(VQA_CSV_PATH)\nabo_meta = pd.read_csv(ABO_META_PATH)\n\n# Merge image paths\nvqa_df = pd.merge(vqa_df, abo_meta[['image_id', 'path']], on='image_id', how='left')\nvqa_df = vqa_df.dropna(subset=[\"path\"]).reset_index(drop=True)\nvqa_df = vqa_df.head(NUM_SAMPLES)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:22:33.036511Z","iopub.execute_input":"2025-05-13T18:22:33.037057Z","iopub.status.idle":"2025-05-13T18:22:33.873460Z","shell.execute_reply.started":"2025-05-13T18:22:33.037031Z","shell.execute_reply":"2025-05-13T18:22:33.872765Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# ===============================\n# INFERENCE FUNCTION\n# ===============================\n\ndef run_inference(model, use_cache: bool, desc: str):\n    answers = []\n    start_time = time.time()\n\n    for _, row in tqdm(vqa_df.iterrows(), total=len(vqa_df), desc=desc):\n        image_path = os.path.join(ABO_IMAGE_PATH, row['path'])\n        question = row['question']\n\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception:\n            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n\n        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            output_ids = model.generate(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                pixel_values=inputs[\"pixel_values\"],\n                use_cache=use_cache,\n                max_length=32,\n                num_beams=1,\n                do_sample=False\n            )\n\n        answer = processor.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        answers.append(answer)\n\n    total_time = time.time() - start_time\n    avg_time = total_time / len(vqa_df)\n    return answers, avg_time\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:22:39.787606Z","iopub.execute_input":"2025-05-13T18:22:39.788223Z","iopub.status.idle":"2025-05-13T18:22:39.795407Z","shell.execute_reply.started":"2025-05-13T18:22:39.788175Z","shell.execute_reply":"2025-05-13T18:22:39.794525Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# ===============================\n# LOAD DATA\n# ===============================\n\nvqa_df = pd.read_csv(VQA_CSV_PATH)\nabo_meta = pd.read_csv(ABO_META_PATH)\nvqa_df = pd.merge(vqa_df, abo_meta[['image_id', 'path']], on='image_id', how='left')\nvqa_df = vqa_df.dropna(subset=[\"path\"]).reset_index(drop=True)\nvqa_df = vqa_df.head(NUM_SAMPLES)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:22:41.623942Z","iopub.execute_input":"2025-05-13T18:22:41.624518Z","iopub.status.idle":"2025-05-13T18:22:42.409970Z","shell.execute_reply.started":"2025-05-13T18:22:41.624495Z","shell.execute_reply":"2025-05-13T18:22:42.409116Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# ===============================\n# RUN BENCHMARK FOR STANDARD ATTENTION (Without Flash)\n# ===============================\n\n# Model without FlashAttention (Standard Attention)\nmodel_standard = BlipForQuestionAnswering.from_pretrained(MODEL_PATH).to(device)\nmodel_standard.eval()\n\n# Run Inference\ntorch.cuda.empty_cache()\nanswers_standard, time_standard = run_inference(\n    model_standard, use_cache=True, desc=\"Standard Attention (No Flash)\"\n)\n\n# ===============================\n# PRINT RESULTS\n# ===============================\nprint(\"\\n==============================\")\nprint(f\"Average time per sample (Standard Attention without Flash): {time_standard:.4f} seconds\")\nprint(\"==============================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:22:43.755308Z","iopub.execute_input":"2025-05-13T18:22:43.755623Z","iopub.status.idle":"2025-05-13T18:41:53.994516Z","shell.execute_reply.started":"2025-05-13T18:22:43.755600Z","shell.execute_reply":"2025-05-13T18:41:53.993815Z"}},"outputs":[{"name":"stderr","text":"Standard Attention (No Flash): 100%|██████████| 10000/10000 [19:08<00:00,  8.71it/s]","output_type":"stream"},{"name":"stdout","text":"\n==============================\nAverage time per sample (Standard Attention without Flash): 0.1148 seconds\n==============================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# ===============================\n# RUN BENCHMARK FOR FLASHATTENTION (With Flash)\n# ===============================\n\n# Model with FlashAttention (using torch.compile)\nmodel_flash = BlipForQuestionAnswering.from_pretrained(MODEL_PATH).to(device)\nmodel_flash.eval()\n\n# Run Inference\ntorch.cuda.empty_cache()\nanswers_flash, time_flash = run_inference(\n    model_flash, use_cache=False, desc=\"FlashAttention with torch.compile\"\n)\n\n# ===============================\n# PRINT RESULTS\n# ===============================\nprint(\"\\n==============================\")\nprint(f\"Average time per sample (FlashAttention enabled): {time_flash:.4f} seconds\")\nprint(\"==============================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:51:12.584133Z","iopub.execute_input":"2025-05-13T18:51:12.584472Z","iopub.status.idle":"2025-05-13T19:09:17.344816Z","shell.execute_reply.started":"2025-05-13T18:51:12.584452Z","shell.execute_reply":"2025-05-13T19:09:17.344177Z"}},"outputs":[{"name":"stderr","text":"FlashAttention with torch.compile: 100%|██████████| 10000/10000 [18:02<00:00,  9.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n==============================\nAverage time per sample (FlashAttention enabled): 0.1083 seconds\n==============================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# ===============================\n# PLOT COMPARISON BETWEEN STANDARD AND FLASHATTENTION\n# ===============================\n\nmethods = [\"Standard Attention\", \"FlashAttention\"]\ntimes = [time_standard, time_flash]\n\nplt.bar(methods, times, color=[\"red\", \"green\"])\nplt.ylabel(\"Avg Inference Time (sec/sample)\")\nplt.title(\"BLIP VQA: Inference Speed Comparison\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:14:42.818565Z","iopub.execute_input":"2025-05-13T19:14:42.818869Z","iopub.status.idle":"2025-05-13T19:14:42.962131Z","shell.execute_reply.started":"2025-05-13T19:14:42.818848Z","shell.execute_reply":"2025-05-13T19:14:42.961431Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# ===============================\n# PRINT SUMMARY OF RESULTS\n# ===============================\n\nprint(\"\\n==============================\")\nprint(f\"Average time per sample (Standard Attention without Flash): {time_standard:.4f} seconds\")\nprint(f\"Average time per sample (FlashAttention enabled): {time_flash:.4f} seconds\")\nprint(\"==============================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:15:04.912530Z","iopub.execute_input":"2025-05-13T19:15:04.912847Z","iopub.status.idle":"2025-05-13T19:15:04.917851Z","shell.execute_reply.started":"2025-05-13T19:15:04.912827Z","shell.execute_reply":"2025-05-13T19:15:04.917139Z"}},"outputs":[{"name":"stdout","text":"\n==============================\nAverage time per sample (Standard Attention without Flash): 0.1148 seconds\nAverage time per sample (FlashAttention enabled): 0.1083 seconds\n==============================\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"torch.cuda.empty_cache()\nanswers_flash, time_flash = run_inference(model, use_cache=False, desc=\"FlashAttention via torch.compile\")\n\nprint(\"\\n==============================\")\nprint(f\"Average time per sample (FlashAttention enabled): {time_flash:.4f} seconds\")\nprint(\"==============================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:06:18.336015Z","iopub.execute_input":"2025-05-13T18:06:18.336356Z","iopub.status.idle":"2025-05-13T18:08:14.426237Z","shell.execute_reply.started":"2025-05-13T18:06:18.336316Z","shell.execute_reply":"2025-05-13T18:08:14.425531Z"}},"outputs":[{"name":"stderr","text":"FlashAttention via torch.compile: 100%|██████████| 1000/1000 [01:56<00:00,  8.61it/s]","output_type":"stream"},{"name":"stdout","text":"\n==============================\nAverage time per sample (FlashAttention enabled): 0.1161 seconds\n==============================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Check GPU utilization during inference\nimport torch\n\ndef print_gpu_usage():\n    if torch.cuda.is_available():\n        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9} GB\")\n        print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1e9} GB\")\n        print(f\"GPU Utilization: {torch.cuda.get_device_properties(0).name}\")\n    else:\n        print(\"CUDA is not available.\")\n\n# Call this function during inference to observe GPU behavior\nprint_gpu_usage()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:21:44.745313Z","iopub.execute_input":"2025-05-13T18:21:44.745626Z","iopub.status.idle":"2025-05-13T18:21:44.751993Z","shell.execute_reply.started":"2025-05-13T18:21:44.745607Z","shell.execute_reply":"2025-05-13T18:21:44.751231Z"}},"outputs":[{"name":"stdout","text":"GPU Memory Allocated: 4.664432128 GB\nGPU Memory Cached: 5.098176512 GB\nGPU Utilization: Tesla T4\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom PIL import Image\nimport pandas as pd\nimport os\nimport time\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:16:32.995182Z","iopub.execute_input":"2025-05-14T07:16:32.995494Z","iopub.status.idle":"2025-05-14T07:16:32.999721Z","shell.execute_reply.started":"2025-05-14T07:16:32.995474Z","shell.execute_reply":"2025-05-14T07:16:32.999006Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Define paths\nabo_small_images_path = \"/kaggle/input/abo-small/small\"\nvqa_csv_path = \"/kaggle/input/vqadataset/VQADataset.csv\"\nfine_tuned_model_path = \"/kaggle/input/finetuning/fine_tuned_blip_vqa_lora\"\n\n# Load VQA dataset (optional)\nvqa_df = pd.read_csv(vqa_csv_path)\ndisplay(vqa_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:16:40.109729Z","iopub.execute_input":"2025-05-14T07:16:40.110434Z","iopub.status.idle":"2025-05-14T07:16:40.229952Z","shell.execute_reply.started":"2025-05-14T07:16:40.110410Z","shell.execute_reply":"2025-05-14T07:16:40.229232Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"      image_id                                           question  \\\n0  41PBKVD918L             What is the primary color of the swing   \n1  41PBKVD918L  What material appears to be used for the swing...   \n2  41PBKVD918L           What is the main purpose of this product   \n3  4129yreLAlL        What is the primary color of the phone case   \n4  4129yreLAlL    What is the apparent material of the phone case   \n\n        answer difficulty  \n0        Green       Easy  \n1        Nylon     Medium  \n2     Relaxing       Hard  \n3  Transparent       Easy  \n4      Plastic     Medium  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>difficulty</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41PBKVD918L</td>\n      <td>What is the primary color of the swing</td>\n      <td>Green</td>\n      <td>Easy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>41PBKVD918L</td>\n      <td>What material appears to be used for the swing...</td>\n      <td>Nylon</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41PBKVD918L</td>\n      <td>What is the main purpose of this product</td>\n      <td>Relaxing</td>\n      <td>Hard</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4129yreLAlL</td>\n      <td>What is the primary color of the phone case</td>\n      <td>Transparent</td>\n      <td>Easy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4129yreLAlL</td>\n      <td>What is the apparent material of the phone case</td>\n      <td>Plastic</td>\n      <td>Medium</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Load the processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load the fine-tuned model\nmodel_int8 = BlipForQuestionAnswering.from_pretrained(fine_tuned_model_path)\n\n# Apply dynamic quantization to Linear layers\nmodel_int8 = torch.quantization.quantize_dynamic(\n    model_int8, {torch.nn.Linear}, dtype=torch.qint8\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:16:47.545493Z","iopub.execute_input":"2025-05-14T07:16:47.546166Z","iopub.status.idle":"2025-05-14T07:16:53.271741Z","shell.execute_reply.started":"2025-05-14T07:16:47.546137Z","shell.execute_reply":"2025-05-14T07:16:53.271159Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from transformers import BlipForConditionalGeneration\nfrom peft import PeftModel\n\nbase_model = BlipForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip-vqa-base\",\n    load_in_8bit=True,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:20:10.423391Z","iopub.execute_input":"2025-05-14T07:20:10.423751Z","iopub.status.idle":"2025-05-14T07:20:10.601828Z","shell.execute_reply.started":"2025-05-14T07:20:10.423715Z","shell.execute_reply":"2025-05-14T07:20:10.600872Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3709343319.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m base_model = BlipForConditionalGeneration.from_pretrained(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"Salesforce/blip-vqa-base\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4228\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   4229\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4230\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             )\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"],"ename":"ImportError","evalue":"Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`","output_type":"error"}],"execution_count":41},{"cell_type":"code","source":"# Select a sample image manually or based on your CSV\nsample_image_path = os.path.join(abo_small_images_path, \"00/00000529.jpg\")  # Update path if needed\nimage = Image.open(sample_image_path).convert(\"RGB\")\n\n# Define a question for the image\nquestion = \"What is in this image?\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:17:04.706312Z","iopub.execute_input":"2025-05-14T07:17:04.706769Z","iopub.status.idle":"2025-05-14T07:17:04.713910Z","shell.execute_reply.started":"2025-05-14T07:17:04.706749Z","shell.execute_reply":"2025-05-14T07:17:04.713270Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def run_inference(model, processor, image, question):\n    model.eval()\n    device = next(model.parameters()).device\n\n    # Preprocess input\n    inputs = processor(image, question, return_tensors=\"pt\").to(device)\n\n    # Inference with generate()\n    start_time = time.time()\n    with torch.no_grad():\n        generated_ids = model.generate(**inputs)\n        answer = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n    end_time = time.time()\n\n    return answer, end_time - start_time\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:18:30.278042Z","iopub.execute_input":"2025-05-14T07:18:30.278429Z","iopub.status.idle":"2025-05-14T07:18:30.283671Z","shell.execute_reply.started":"2025-05-14T07:18:30.278406Z","shell.execute_reply":"2025-05-14T07:18:30.282922Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"answer_int8, time_int8 = run_inference(model_int8, processor, image, question)\n\n# Cleanup\ndel model_int8\ngc.collect()\n\nprint(f\"Answer: {answer_int8}\")\nprint(f\"Inference Time (INT8 CPU): {time_int8:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:18:36.207666Z","iopub.execute_input":"2025-05-14T07:18:36.208198Z","iopub.status.idle":"2025-05-14T07:18:36.964011Z","shell.execute_reply.started":"2025-05-14T07:18:36.208174Z","shell.execute_reply":"2025-05-14T07:18:36.961956Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4225324471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manswer_int8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_int8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_int8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel_int8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3362216754.py\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(model, processor, image, question)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, pixel_values, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         question_outputs = self.text_encoder(\n\u001b[0m\u001b[1;32m   1433\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder)\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 )\n\u001b[1;32m    435\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    437\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 275\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_dropout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_adapter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mscaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_adapter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_adapter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'dtype'"],"ename":"AttributeError","evalue":"'function' object has no attribute 'dtype'","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"from torch._dynamo import optimize\n\nmodel_flash = BlipForQuestionAnswering.from_pretrained(MODEL_PATH).to(device).half()\nmodel_flash = torch.compile(model_flash, mode=\"reduce-overhead\")\n\ntorch.cuda.empty_cache()\nanswers_flash, time_flash = run_inference(\n    model_flash, processor, use_cache=False, desc=\"FlashAttention (FP16)\"\n)\n\ndel model_flash\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:38:28.515689Z","iopub.execute_input":"2025-05-14T06:38:28.516394Z","iopub.status.idle":"2025-05-14T06:40:07.555461Z","shell.execute_reply.started":"2025-05-14T06:38:28.516368Z","shell.execute_reply":"2025-05-14T06:40:07.554932Z"}},"outputs":[{"name":"stderr","text":"FlashAttention (FP16): 100%|██████████| 1000/1000 [01:36<00:00, 10.32it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import requests\n\nmodel_int8 = BlipForQuestionAnswering.from_pretrained(MODEL_PATH)\nmodel_int8 = torch.quantization.quantize_dynamic(\n    model_int8, {torch.nn.Linear}, dtype=torch.qint8\n)\ndevice_cpu = torch.device(\"cpu\")\n\ntorch.cuda.empty_cache()\nanswers_int8, time_int8 = run_inference(\n    model_int8.to(device_cpu), processor, use_cache=False, desc=\"Quantized INT8 (CPU)\"\n)\n\ndel model_int8\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:46:20.047992Z","iopub.execute_input":"2025-05-14T06:46:20.048537Z","iopub.status.idle":"2025-05-14T06:46:25.188584Z","shell.execute_reply.started":"2025-05-14T06:46:20.048513Z","shell.execute_reply":"2025-05-14T06:46:25.187519Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1858723955.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m answers_int8, time_int8 = run_inference(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_int8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Quantized INT8 (CPU)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n","\u001b[0;32m/tmp/ipykernel_35/4097044889.py\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(model, processor, use_cache, desc)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/salesforce/BLIP/main/demo.jpg\"\u001b[0m  \u001b[0;31m# sample image from BLIP repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3530\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3531\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3532\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7f911c964810>"],"ename":"UnidentifiedImageError","evalue":"cannot identify image file <_io.BytesIO object at 0x7f911c964810>","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"pip install Pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:00:55.241672Z","iopub.execute_input":"2025-05-14T07:00:55.242235Z","iopub.status.idle":"2025-05-14T07:00:58.413266Z","shell.execute_reply.started":"2025-05-14T07:00:55.242197Z","shell.execute_reply":"2025-05-14T07:00:58.412429Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import requests\nfrom io import BytesIO\nfrom PIL import Image\n\nurl = \"https://raw.githubusercontent.com/salesforce/BLIP/main/demo.jpg\"\n\n# Fetch the image\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\nelse:\n    print(f\"Failed to retrieve image, HTTP Status Code: {response.status_code}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:01:46.141257Z","iopub.execute_input":"2025-05-14T07:01:46.141542Z","iopub.status.idle":"2025-05-14T07:01:46.188098Z","shell.execute_reply.started":"2025-05-14T07:01:46.141519Z","shell.execute_reply":"2025-05-14T07:01:46.187568Z"}},"outputs":[{"name":"stdout","text":"Failed to retrieve image, HTTP Status Code: 404\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import torch\nimport gc\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom PIL import Image\nimport pandas as pd\nimport os\nimport requests\n\n# Define paths for the input data\nabo_small_images_path = \"/kaggle/input/abo-small/small\"  # Folder with images\nvqa_csv_path = \"/kaggle/input/vqadataset/VQADataset.csv\"  # VQA dataset CSV path\nfine_tuned_model_path = \"/kaggle/input/finetuning/fine_tuned_blip_vqa_lora\"  # Fine-tuned model path\n\n# Load the processor for the BLIP model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load the INT8 quantized model\nmodel_int8 = BlipForQuestionAnswering.from_pretrained(fine_tuned_model_path)\n\n# Apply dynamic quantization to the model (Quantizing Linear layers to INT8)\nmodel_int8 = torch.quantization.quantize_dynamic(\n    model_int8, {torch.nn.Linear}, dtype=torch.qint8\n)\n\n# Set the device to CPU for running the quantized model\ndevice_cpu = torch.device(\"cpu\")\n\n# Load the VQA dataset (optional)\nvqa_df = pd.read_csv(vqa_csv_path)\nprint(vqa_df.head())  # Print the first few rows of the dataset\n\n# Select a sample image for inference (you can replace this with any image path)\nsample_image_path = os.path.join(abo_small_images_path, \"00/00000529.jpg\")  # Example image name\nimage = Image.open(sample_image_path).convert(\"RGB\")\n\n# Define the question for VQA\nquestion = \"What is in this image?\"\n\n# Prepare inputs using the BLIP processor\ninputs = processor(images=image, text=question, return_tensors=\"pt\")\n\n# Move the inputs to the CPU (already on CPU)\ninputs = {k: v.to(device_cpu) for k, v in inputs.items()}\nmodel_int8 = model_int8.to(device_cpu)\n\n# Perform inference\ntorch.cuda.empty_cache()\nanswers_int8, time_int8 = run_inference(\n    model_int8, processor, use_cache=False, desc=\"Quantized INT8 (CPU)\"\n)\n\n# Clean up after inference\ndel model_int8\ngc.collect()\n\n# Print the answer and inference time\nprint(f\"Answer: {answers_int8}\")\nprint(f\"Inference Time: {time_int8}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:59:53.985720Z","iopub.execute_input":"2025-05-14T06:59:53.986490Z","iopub.status.idle":"2025-05-14T06:59:58.606565Z","shell.execute_reply.started":"2025-05-14T06:59:53.986457Z","shell.execute_reply":"2025-05-14T06:59:58.605659Z"}},"outputs":[{"name":"stdout","text":"      image_id                                           question  \\\n0  41PBKVD918L             What is the primary color of the swing   \n1  41PBKVD918L  What material appears to be used for the swing...   \n2  41PBKVD918L           What is the main purpose of this product   \n3  4129yreLAlL        What is the primary color of the phone case   \n4  4129yreLAlL    What is the apparent material of the phone case   \n\n        answer difficulty  \n0        Green       Easy  \n1        Nylon     Medium  \n2     Relaxing       Hard  \n3  Transparent       Easy  \n4      Plastic     Medium  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2657264294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Perform inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m answers_int8, time_int8 = run_inference(\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mmodel_int8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Quantized INT8 (CPU)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m )\n","\u001b[0;32m/tmp/ipykernel_35/4097044889.py\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(model, processor, use_cache, desc)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/salesforce/BLIP/main/demo.jpg\"\u001b[0m  \u001b[0;31m# sample image from BLIP repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3530\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3531\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3532\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7f91347abbf0>"],"ename":"UnidentifiedImageError","evalue":"cannot identify image file <_io.BytesIO object at 0x7f91347abbf0>","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# List only file names, without printing full paths for every file\ninput_dir = '/kaggle/input'\nfilenames = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n\n# Print the filenames in the input directory\nfor filename in filenames:\n    print(filename)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:35:30.399223Z","iopub.execute_input":"2025-05-13T17:35:30.399757Z","iopub.status.idle":"2025-05-13T17:35:30.404062Z","shell.execute_reply.started":"2025-05-13T17:35:30.399732Z","shell.execute_reply":"2025-05-13T17:35:30.403434Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"pip install flash-attn --no-build-isolation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:35:33.411068Z","iopub.execute_input":"2025-05-13T17:35:33.411641Z","iopub.status.idle":"2025-05-13T17:37:08.928386Z","shell.execute_reply.started":"2025-05-13T17:35:33.411616Z","shell.execute_reply":"2025-05-13T17:37:08.927529Z"}},"outputs":[{"name":"stdout","text":"Collecting flash-attn\n  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash-attn)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash-attn)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash-attn)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->flash-attn)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash-attn)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash-attn)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash-attn)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187831595 sha256=58853b28a5a926cae14402bfd8d4d93a45ebf8f9e79533f37ab09d0d77a99c05\n  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\nSuccessfully built flash-attn\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed flash-attn-2.7.4.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"pip install flash-attn --no-build-isolation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:56:47.854987Z","iopub.execute_input":"2025-05-13T17:56:47.855283Z","iopub.status.idle":"2025-05-13T17:58:27.478965Z","shell.execute_reply.started":"2025-05-13T17:56:47.855261Z","shell.execute_reply":"2025-05-13T17:58:27.478081Z"}},"outputs":[{"name":"stdout","text":"Collecting flash-attn\n  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash-attn)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash-attn)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash-attn)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->flash-attn)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash-attn)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash-attn)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash-attn)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187831595 sha256=58853b28a5a926cae14402bfd8d4d93a45ebf8f9e79533f37ab09d0d77a99c05\n  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\nSuccessfully built flash-attn\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed flash-attn-2.7.4.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from flash_attn.modules.mha import FlashMHA\n\ndef replace_attention_with_flash(model):\n    for layer in model.text_decoder.bert.encoder.layer:\n        orig_attention = layer.attention.self\n\n        # Initialize FlashAttention MultiheadAttention\n        flash_mha = FlashMHA(\n            embed_dim=orig_attention.query.in_features,\n            num_heads=orig_attention.num_attention_heads,\n            bias=True,\n            dropout=orig_attention.dropout.p,\n            batch_first=True\n        ).to(device)\n\n        # Transfer weights from original attention to FlashMHA\n        with torch.no_grad():\n            # Copy QKV weights and biases\n            flash_mha.qkv.weight.copy_(torch.cat([\n                orig_attention.query.weight,\n                orig_attention.key.weight,\n                orig_attention.value.weight\n            ], dim=0))\n            flash_mha.qkv.bias.copy_(torch.cat([\n                orig_attention.query.bias,\n                orig_attention.key.bias,\n                orig_attention.value.bias\n            ], dim=0))\n            # Copy output projection\n            flash_mha.out_proj.weight.copy_(orig_attention.output_dense.weight)\n            flash_mha.out_proj.bias.copy_(orig_attention.output_dense.bias)\n\n        # Replace the original attention module with FlashAttention\n        layer.attention.self = flash_mha\n\n    return model\n\n# Create FlashAttention-enabled model instance\nmodel_flash = replace_attention_with_flash(\n    BlipForQuestionAnswering.from_pretrained(MODEL_PATH)\n).to(device)\nmodel_flash.eval()\n\n# Run FlashAttention benchmark\ntorch.cuda.empty_cache()\nanswers_flash, time_flash = run_inference(\n    model_flash, use_cache=False, desc=\"FlashAttention\"\n)\n\n# Print performance\nprint(f\"\\nFlashAttention Avg Time: {time_flash:.4f} sec/sample\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:56:04.902386Z","iopub.execute_input":"2025-05-13T17:56:04.903214Z","iopub.status.idle":"2025-05-13T17:56:04.947334Z","shell.execute_reply.started":"2025-05-13T17:56:04.903181Z","shell.execute_reply":"2025-05-13T17:56:04.946415Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_67/1311698957.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflash_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlashMHA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreplace_attention_with_flash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0morig_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'FlashMHA' from 'flash_attn.modules.mha' (/usr/local/lib/python3.11/dist-packages/flash_attn/modules/mha.py)"],"ename":"ImportError","evalue":"cannot import name 'FlashMHA' from 'flash_attn.modules.mha' (/usr/local/lib/python3.11/dist-packages/flash_attn/modules/mha.py)","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom torch import nn\nfrom tqdm import tqdm\n\n# ===============================\n# CONFIGURATION\n# ===============================\n\nMODEL_PATH = \"/kaggle/input/finetuning/fine_tuned_blip_vqa_lora\"\nVQA_CSV_PATH = \"/kaggle/input/vqadataset/VQADataset.csv\"\nABO_META_PATH = \"/kaggle/input/abo-small/metadata/images.csv\"\nABO_IMAGE_PATH = \"/kaggle/input/abo-small/small\"\nNUM_SAMPLES = 1000  # Reduced for faster testing; adjust as needed\n\n# ===============================\n# LOAD MODEL & PROCESSOR\n# ===============================\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(MODEL_PATH)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Modify BLIP model to use torch.nn.MultiheadAttention with torch.compile\nfrom flash_attn.modules.mha import FlashMHA\n\ndef replace_attention_with_flash(model):\n    for layer in model.text_decoder.bert.encoder.layer:\n        orig_attention = layer.attention.self\n\n        # Use FlashMHA (batch_first=True for HuggingFace compatibility)\n        flash_mha = FlashMHA(\n            embed_dim=orig_attention.query.in_features,\n            num_heads=orig_attention.num_attention_heads,\n            bias=True,\n            dropout=orig_attention.dropout.p,\n            batch_first=True\n        ).to(device)\n\n        # Copy weights\n        with torch.no_grad():\n            # FlashMHA uses a single qkv weight matrix\n            flash_mha.qkv.weight.copy_(torch.cat([\n                orig_attention.query.weight,\n                orig_attention.key.weight,\n                orig_attention.value.weight\n            ], dim=0))\n            flash_mha.qkv.bias.copy_(torch.cat([\n                orig_attention.query.bias,\n                orig_attention.key.bias,\n                orig_attention.value.bias\n            ]))\n            flash_mha.out_proj.weight.copy_(orig_attention.output_dense.weight)\n            flash_mha.out_proj.bias.copy_(orig_attention.output_dense.bias)\n\n        # Replace attention with FlashAttention\n        layer.attention.self = flash_mha\n\n    return model\n\n\n# Create two model instances for benchmarking\nmodel_standard = BlipForQuestionAnswering.from_pretrained(MODEL_PATH).to(device)\nmodel_optimized = replace_attention_with_torch(BlipForQuestionAnswering.from_pretrained(MODEL_PATH)).to(device)\n\nmodel_standard.eval()\nmodel_optimized.eval()\n\n# ===============================\n# LOAD DATA\n# ===============================\n\nvqa_df = pd.read_csv(VQA_CSV_PATH)\nabo_meta = pd.read_csv(ABO_META_PATH)\nvqa_df = pd.merge(vqa_df, abo_meta[['image_id', 'path']], on='image_id', how='left')\nvqa_df = vqa_df.dropna(subset=[\"path\"]).reset_index(drop=True)\nvqa_df = vqa_df.head(NUM_SAMPLES)\n\n# ===============================\n# INFERENCE FUNCTION\n# ===============================\n\ndef run_inference(model, use_cache: bool, desc: str):\n    answers = []\n    start_time = time.time()\n\n    for _, row in tqdm(vqa_df.iterrows(), total=len(vqa_df), desc=desc):\n        image_path = os.path.join(ABO_IMAGE_PATH, row['path'])\n        question = row['question']\n\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception:\n            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n\n        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            output_ids = model.generate(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                pixel_values=inputs[\"pixel_values\"],\n                use_cache=use_cache,\n                max_length=32,\n                num_beams=1,\n                do_sample=False\n            )\n\n        answer = processor.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        answers.append(answer)\n\n    total_time = time.time() - start_time\n    avg_time = total_time / len(vqa_df)\n    return answers, avg_time\n\n# ===============================\n# RUN BENCHMARK\n# ===============================\n\n# Standard attention with KV cache\ntorch.cuda.empty_cache()\nanswers_standard, time_standard = run_inference(\n    model_standard, use_cache=True, desc=\"Standard Attention with KV Cache\"\n)\n\n# Optimized attention (torch.compile, no KV cache)\ntorch.cuda.empty_cache()\nanswers_optimized, time_optimized = run_inference(\n    model_optimized, use_cache=False, desc=\"Optimized Attention (torch.compile)\"\n)\n\n# ===============================\n# RESULTS\n# ===============================\n\nnum_same = sum(a1 == a2 for a1, a2 in zip(answers_standard, answers_optimized))\n\nprint(\"\\n==============================\")\nprint(f\"Average time with Standard Attention (KV Cache): {time_standard:.4f} seconds/sample\")\nprint(f\"Average time with Optimized Attention (torch.compile): {time_optimized:.4f} seconds/sample\")\nprint(f\"Answer consistency: {num_same}/{NUM_SAMPLES} identical\")\nprint(\"==============================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:52:39.727433Z","iopub.execute_input":"2025-05-13T17:52:39.728062Z","iopub.status.idle":"2025-05-13T17:52:44.967816Z","shell.execute_reply.started":"2025-05-13T17:52:39.728038Z","shell.execute_reply":"2025-05-13T17:52:44.966758Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_67/708350136.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Create two model instances for benchmarking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mmodel_standard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlipForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mmodel_optimized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_attention_with_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBlipForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mmodel_standard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_67/708350136.py\u001b[0m in \u001b[0;36mreplace_attention_with_torch\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     52\u001b[0m             ]))\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Corrected: Use output_dense instead of output.dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mmha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mmha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Replace attention module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'BlipTextSelfAttention' object has no attribute 'output_dense'"],"ename":"AttributeError","evalue":"'BlipTextSelfAttention' object has no attribute 'output_dense'","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"from transformers import BlipForQuestionAnswering, BlipProcessor\nfrom peft import PeftModel, PeftConfig\nimport torch\n\nbase_path = \"/kaggle/input/finetuning/fine_tuned_blip_vqa_lora\"\n\n# Load processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load PEFT config and model\npeft_config = PeftConfig.from_pretrained(base_path)\nbase_model = BlipForQuestionAnswering.from_pretrained(peft_config.base_model_name_or_path)\nmodel = PeftModel.from_pretrained(base_model, base_path)\nmodel.eval()\n\nprint(\"LoRA fine-tuned model loaded successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply dynamic quantization (only affects CPU, shown for demonstration)\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\n\nprint(\"Applied dynamic quantization.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load your VQA dataset\ncsv_path = \"/kaggle/input/vqadataset/VQADataset.csv\"\ntest_df = pd.read_csv(csv_path)\n\n# Show 5 random samples to inspect\ntest_df.sample(5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test_df.columns)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Check contents of the 'small' directory\ndirectory_path = \"/kaggle/input/abo-small/small\"\nif os.path.exists(directory_path):\n    print(f\"Contents of {directory_path}:\")\n    print(os.listdir(directory_path))\nelse:\n    print(f\"Directory {directory_path} not found.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check contents of a specific subdirectory\nsubdirectory_path = \"/kaggle/input/abo-small/small/81\"  # Replace with any subdirectory you want to check\nif os.path.exists(subdirectory_path):\n    print(f\"Contents of {subdirectory_path}:\")\n    print(os.listdir(subdirectory_path))\nelse:\n    print(f\"Directory {subdirectory_path} not found.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_id = sample_row['image_id']  # From your VQA dataset\nimage_path = f\"/kaggle/input/abo-small/small/81/{image_id}.jpg\"  # Build the correct image path\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\n# Select a sample row from the dataframe\nsample_row = test_df.sample(1).iloc[0]  # Randomly select a row\n\n# Get image_id and question\nimage_id = sample_row['image_id']\nquestion = sample_row['question']\n\n# Build the full image path\nimage_path = f\"/kaggle/input/abo-small/small/{image_id}/{image_id}_0.jpg\"\n\n# Check if the image exists\nif os.path.exists(image_path):\n    print(f\"Image found at {image_path}. Proceeding with processing.\")\n    # Load and process the image\n    image = Image.open(image_path).convert(\"RGB\")\n    print(\"Image loaded successfully!\")\nelse:\n    print(f\"Image not found at {image_path}. Checking the directory structure.\")\n    # List contents of the directory to find any discrepancies\n    directory_path = f\"/kaggle/input/abo-small/small/{image_id}\"\n    if os.path.exists(directory_path):\n        print(f\"Contents of {directory_path}:\")\n        print(os.listdir(directory_path))\n    else:\n        print(f\"Directory {directory_path} not found.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport torch\n\n# Sample image path and question\nimage_path = \"/kaggle/input/abo-small/small/110101006/110101006_0.jpg\"  # Change this to a valid ABO image\nquestion = \"What color is the product?\"\n\n# Load and process the image\nimage = Image.open(image_path).convert(\"RGB\")\n\n# Preprocess input\ninputs = processor(image, question, return_tensors=\"pt\").to(device)\n\n# Generate answer\nwith torch.no_grad():\n    out = model.generate(**inputs, max_length=32)\n    answer = processor.tokenizer.decode(out[0], skip_special_tokens=True)\n\nprint(\"🔍 Question:\", question)\nprint(\"💬 Answer:\", answer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*stating new here*****","metadata":{}},{"cell_type":"code","source":"# Step 1: Import necessary libraries\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport torch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Load your VQA dataset\ncsv_path = \"/kaggle/input/vqadataset/VQADataset.csv\"\ntest_df = pd.read_csv(csv_path)\n\n# Show a sample of the dataset to understand its structure\nprint(\"Dataset Sample:\")\nprint(test_df.sample(5))  # This will print 5 random rows to inspect the dataset\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Initialize the BLIP processor and model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Pick a sample row to test\nsample_row = test_df.iloc[0]  # Change index to test different rows\nimage_id = sample_row['image_id']\nquestion = sample_row['question']\n\n# Construct the image path based on the image_id\nimage_path = f\"/kaggle/input/abo-small/small/{image_id[:2]}/{image_id}.jpg\"  # Adjust path based on image_id\n\nprint(\"Image Path:\", image_path)  # Print the path to verify\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Check if the image exists in the specified path\nif os.path.exists(image_path):\n    print(f\"Image found: {image_path}\")\nelse:\n    print(f\"Image not found at: {image_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering, Trainer, TrainingArguments\nimport torch\nfrom accelerate import Accelerator\nfrom sklearn.model_selection import train_test_split\nfrom peft import LoraConfig, get_peft_model\nfrom transformers.data.data_collator import default_data_collator\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Accelerator for efficient multi-GPU training\naccelerator = Accelerator()\n\n# Load BLIP-1 VQA processor and model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", use_fast=True)\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Accelerator for efficient multi-GPU training\naccelerator = Accelerator()\n\n# Load BLIP-1 VQA processor and model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", use_fast=True)\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define dataset paths (adjust these to your Kaggle environment)\nVQA_DATASET_PATH = '/kaggle/input/vqadataset/VQADataset.csv'\nABO_METADATA_PATH = '/kaggle/input/abo-small/metadata/images.csv'\nABO_IMAGE_BASE_PATH = '/kaggle/input/abo-small/small'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load VQA dataset\nvqa_df = pd.read_csv(VQA_DATASET_PATH)\nprint(f\"Loaded VQA dataset with {len(vqa_df)} entries\")\n\n# Load ABO metadata and merge to get image paths\nabo_metadata = pd.read_csv(ABO_METADATA_PATH)\nvqa_df = pd.merge(vqa_df, abo_metadata[['image_id', 'path']], on='image_id', how='left')\n\n# Handle missing values and ensure answers are strings\nvqa_df['answer'] = vqa_df['answer'].fillna('unknown').astype(str)\n\n# Split data into train and test sets\ntrain_df, test_df = train_test_split(vqa_df, test_size=0.2, random_state=118)\nprint(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VQADataset(torch.utils.data.Dataset):\n    def __init__(self, df, processor, image_base_path):\n        self.df = df\n        self.processor = processor\n        self.image_base_path = image_base_path\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_base_path, row['path'])\n        \n        # Ensure the image exists before loading\n        if not os.path.exists(image_path):\n            print(f\"Image not found: {image_path}\")\n            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))  # Placeholder for missing image\n        else:\n            image = Image.open(image_path).convert(\"RGB\")\n\n        question = row['question']\n        answer = row['answer']\n\n        # Process image and question with attention mask\n        encoding = self.processor(\n            images=image,\n            text=question,\n            padding=\"max_length\",\n            max_length=128,\n            truncation=True,\n            return_tensors=\"pt\",\n            return_attention_mask=True\n        )\n\n        # Tokenize answer as labels with fixed length\n        labels = self.processor.tokenizer(\n            answer,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=32,\n            return_tensors=\"pt\"\n        )[\"input_ids\"]\n\n        # Remove batch dimension from tensors\n        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n        encoding[\"labels\"] = labels.squeeze(0)\n\n        return encoding\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create training dataset\ntrain_dataset = VQADataset(train_df, processor, ABO_IMAGE_BASE_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define LoRA configuration\nlora_config = LoraConfig(\n    r=16,  # Rank of low-rank matrices\n    lora_alpha=32,  # Scaling factor\n    target_modules=[\"query\", \"value\"],  # Target attention layers in BLIP-1\n    lora_dropout=0.1,  # Dropout for regularization\n    bias=\"none\"  # No bias adaptation\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\nprint(\"LoRA applied to the model\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare model with Accelerator\nmodel = accelerator.prepare(model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    run_name=\"blip_vqa_lora_finetune\",  # Unique run name\n    num_train_epochs=3,\n    per_device_train_batch_size=4,  # Reduced for memory stability\n    gradient_accumulation_steps=4,  # Simulate larger batch size (effective batch size = 16)\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    fp16=True,  # Mixed precision for efficiency\n    remove_unused_columns=False,  # Keep all dataset columns\n    report_to=\"none\"  # Disable W&B and other logging integrations\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Trainer instance with default data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=default_data_collator,  # Handle tensor stacking\n)\n\n# Check GPU memory usage before training\nif torch.cuda.is_available():\n    print(\"GPU Memory Usage Before Training:\")\n    print(torch.cuda.memory_summary())\n\n# Start fine-tuning with LoRA\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.save_model(\"./fine_tuned_blip_vqa_lora\")\nprint(\"Model saved to './fine_tuned_blip_vqa_lora'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}